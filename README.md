# TP2 â€” Modern Data Pipeline

*PostgreSQL â†’ S3 â†’ Snowflake â†’ dbt â†’ Airflow â†’ Power BI*

## ğŸ“Œ Description

This TP implements a full cloud-based Modern Data Stack pipeline using the **ShopStream** e-commerce scenario.

Technologies used:

* **PostgreSQL** â€” OLTP database
* **Amazon S3** â€” Data Lake (RAW Zone)
* **Snowflake** â€” Cloud Data Warehouse
* **dbt** â€” Transformations as SQL Models
* **Airflow** â€” Orchestration
* **Power BI** â€” BI Visualization

---

## ğŸ“ Project Structure

```
TP2/
â”‚
â”œâ”€â”€ SQL (PostgreSQL schema)
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_data.py
â”‚   â””â”€â”€ extraction_python.py
â”œâ”€â”€ dbt/
â”œâ”€â”€ airflow/
â”œâ”€â”€ powerbi/
â””â”€â”€ README.md
```

---

## ğŸ§± Step 1 â€” PostgreSQL (OLTP Layer)

Create the operational schema with 6 tables:

* users
* products
* orders
* order_items
* events
* crm_contacts

Run the provided SQL script to initialize the database.

---

## ğŸ§ª Step 2 â€” Generate Sample Data

Python script used:

```
scripts/generate_data.py
```

It generates:

* Users
* Products
* Orders
* Order Items
* CRM contacts
* Events

All inserted directly into PostgreSQL.

---

## ğŸª£ Step 3 â€” S3 Data Lake (RAW Zone)

Using Python:

* Extract OLTP tables
* Save as CSV or Parquet
* Upload to your S3 bucket

Your S3 structure:

```
s3://shopstream-raw/
    users/
    products/
    orders/
    order_items/
    events/
    crm_contacts/
```

---

## â„ Step 4 â€” Snowflake (STAGE + CORE)

### STAGE layer:

Load S3 data using:

```
COPY INTO stage.users
FROM @my_s3_stage/users
FILE_FORMAT = (TYPE = CSV ...)
```

### CORE layer:

Created using dbt transformations:

* Dimensional models
* Fact tables
* Business logic

---

## ğŸ›  Step 5 â€” dbt Transformations

Your dbt project includes:

* `staging` models
* `core` models
* `marts` models (business KPIs)

Run:

```
dbt run
dbt test
```

---

## â± Step 6 â€” Orchestration with Airflow

The Airflow DAG triggers:

1. Extract from PostgreSQL
2. Upload to S3
3. Load Snowflake STAGE
4. Run dbt models
5. Refresh Power BI dataset

File examples:

* `dag_shopstream.py`
* `task_extract.py`
* `task_dbt.py`

---

## ğŸ“Š Step 7 â€” BI with Power BI

Final dashboard includes:

* Revenue by country
* Customer funnel
* CLV
* Top products
* Orders trends

---

## âœ” Deliverables 

* PostgreSQL SQL file
* Python extraction + generation scripts
* S3 folder structure
* Snowflake STAGE + CORE models
* dbt project
* Airflow DAG
* Power BI dashboard (.pbix)
* This README.md

---

