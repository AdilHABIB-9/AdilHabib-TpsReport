# ğŸ“Š TP DATA WAREHOUSE - Lakehouse avec Delta Lake

## ğŸ“‹ Description du Projet
Pipeline ETL complet pour crÃ©er un data warehouse moderne avec architecture Lakehouse (Bronze-Silver-Gold) en utilisant:
- **PostgreSQL** comme source de donnÃ©es
- **Apache Spark** pour le traitement
- **Delta Lake** pour le stockage
- **Python/PySpark** pour l'orchestration

## ğŸ—ï¸ Architecture

PostgreSQL â†’ Bronze (raw) â†’ Silver (cleaned) â†’ Gold (aggregated) â†’ Rapports

## ğŸ“ Structure des Fichiers

TP_DataWarehouse/
â”œâ”€â”€ drivers/ # Drivers JDBC
â”‚ â””â”€â”€ postgresql-42.7.1.jar # Driver PostgreSQL
â”œâ”€â”€ venv/ # Environnement virtuel Python
â”œâ”€â”€ 05_bronze_ingestion.py # Ingestion PostgreSQL â†’ Bronze
â”œâ”€â”€ 06_verify_bronze.py # VÃ©rification couche Bronze
â”œâ”€â”€ 07_silver_transformation.py # Transformation Bronze â†’ Silver
â”œâ”€â”€ 08_gold_aggregation.py # AggrÃ©gation Silver â†’ Gold
â”œâ”€â”€ 09_generer_rapport.py # GÃ©nÃ©ration rapport final
â”œâ”€â”€ README.md # Ce fichier
â””â”€â”€ requirements.txt # DÃ©pendances Python

## âš™ï¸ PrÃ©-requis

### 1. Installer Python 3.8+
```cmd
python --version
java -version
Important: DÃ©finir JAVA_HOME dans les variables d'environnement.
3. Installer PostgreSQL

TÃ©lÃ©charger: https://www.postgresql.org/download/

CrÃ©er la base: retailpro_dwh

ExÃ©cuter les scripts SQL fournis

4. Configurer l'environnement Windows
# Variables d'environnement Ã  ajouter
HADOOP_HOME = C:\Users\adilh\OneDrive\Desktop\hadoop-3.0.0
JAVA_HOME = C:\Program Files\Eclipse Adoptium\jdk-11.0.28.6-hotspot

# Ajouter au PATH
%HADOOP_HOME%\bin
%JAVA_HOME%\bin

ğŸš€ Installation
1. Cloner/Initialiser le projet
cmd
cd C:\Users\adilh\OneDrive\Desktop\TP_DataWarehouse
2. CrÃ©er l'environnement virtuel
cmd
python -m venv venv
3. Activer l'environnement virtuel
# Windows
venv\Scripts\activate

# Linux/Mac
source venv/bin/activate
4. Installer les dÃ©pendances
pip install -r requirements.txt
Si requirements.txt n'existe pas, installer manuellement:
pip install pyspark==3.5.0
pip install delta-spark==3.0.0
pip install pandas openpyxl


5. TÃ©lÃ©charger le driver PostgreSQL JDBC
# CrÃ©er le dossier drivers
mkdir drivers

# TÃ©lÃ©charger depuis:
https://jdbc.postgresql.org/download/postgresql-42.7.1.jar

# Placer dans:
C:\Users\adilh\OneDrive\Desktop\TP_DataWarehouse\drivers\

ğŸ—„ï¸ Configuration de la Base de DonnÃ©es
1. DÃ©marrer PostgreSQL
# Via pgAdmin ou ligne de commande
psql -U postgres

2. CrÃ©er la base de donnÃ©es
CREATE DATABASE retailpro_dwh;
\c retailpro_dwh;

3. CrÃ©er les tables (exemple)
-- Table clients_source
CREATE TABLE clients_source (
    client_id SERIAL PRIMARY KEY,
    nom VARCHAR(50),
    prenom VARCHAR(50),
    email VARCHAR(100),
    ville VARCHAR(50),
    segment VARCHAR(20),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Table produits_source
CREATE TABLE produits_source (
    produit_id SERIAL PRIMARY KEY,
    nom_produit VARCHAR(100),
    categorie VARCHAR(50),
    prix_unitaire DECIMAL(10,2),
    cout_achat DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- Table ventes_source
CREATE TABLE ventes_source (
    vente_id SERIAL PRIMARY KEY,
    client_id INT REFERENCES clients_source(client_id),
    produit_id INT REFERENCES produits_source(produit_id),
    date_vente TIMESTAMP,
    quantite INT,
    montant_total DECIMAL(10,2),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- InsÃ©rer des donnÃ©es de test
INSERT INTO clients_source (nom, prenom, email, ville, segment) VALUES
('Dupont', 'Jean', 'jean.dupont@email.com', 'Lyon', 'Gold'),
('Martin', 'Marie', 'marie.martin@email.com', 'Lyon', 'Platinum'),
('Bernard', 'Pierre', 'pierre.bernard@email.com', 'Marseille', 'Bronze');

INSERT INTO produits_source (nom_produit, categorie, prix_unitaire, cout_achat) VALUES
('Laptop HP', 'Informatique', 899.99, 650.00),
('Souris Logitech', 'Informatique', 29.99, 15.00),
('Clavier MÃ©canique', 'Informatique', 149.99, 80.00);

INSERT INTO ventes_source (client_id, produit_id, date_vente, quantite, montant_total) VALUES
(1, 1, '2026-01-01 10:00:00', 1, 899.99),
(2, 2, '2026-01-01 11:00:00', 2, 59.98),
(3, 3, '2026-01-01 12:00:00', 1, 149.99);

ğŸƒâ€â™‚ï¸ ExÃ©cution du Pipeline
Ã‰tape 1: Ingestion Bronze
python 05_bronze_ingestion.py
RÃ©sultat: CrÃ©e C:\lakehouse\bronze\ avec 3 tables Delta.

Ã‰tape 2: VÃ©rification Bronze
python 06_verify_bronze.py
VÃ©rifie: DonnÃ©es, schÃ©mas et mÃ©tadonnÃ©es de la couche Bronze.

Ã‰tape 3: Transformation Silver
python 07_silver_transformation.py
Actions: Nettoyage, standardisation, correction encodage.

Ã‰tape 4: AggrÃ©gation Gold
python 08_gold_aggregation.py
CrÃ©e: MÃ©triques business (ventes quotidiennes, etc.).

Ã‰tape 5: GÃ©nÃ©ration Rapport
python 09_generer_rapport.py
Affiche: Statistiques globales et KPIs.

ğŸ“Š RÃ©sultats Attendus
Structure des DonnÃ©es

C:\lakehouse\
â”œâ”€â”€ bronze\          # DonnÃ©es brutes (raw)
â”‚   â”œâ”€â”€ clients\     # clients_source â†’ clients
â”‚   â”œâ”€â”€ produits\    # produits_source â†’ produits
â”‚   â””â”€â”€ ventes\      # ventes_source â†’ ventes
â”œâ”€â”€ silver\          # DonnÃ©es nettoyÃ©es
â”‚   â”œâ”€â”€ clients\     # StandardisÃ© + validation
â”‚   â”œâ”€â”€ produits\    # Encodage fixÃ© + marges
â”‚   â””â”€â”€ ventes\      # DÃ©coupage date + validation
â””â”€â”€ gold\           # MÃ©triques business
    â””â”€â”€ ventes_quotidiennes\  # CA par jour

MÃ©triques CalculÃ©es
âœ… Chiffre d'affaires total

âœ… Nombre de ventes

âœ… Panier moyen

âœ… Top jours par CA

âœ… PÃ©riode analysÃ©e

ğŸ“š Documentation Technique
BibliothÃ¨ques UtilisÃ©es
PySpark 3.5.0: Traitement distribuÃ©

Delta Lake 3.0.0: Stockage transactionnel

PostgreSQL JDBC: Connexion base de donnÃ©es

Concepts ClÃ©s
Lakehouse: Combine data lake + data warehouse

Delta Lake: Stockage ACID sur object storage

Medallion Architecture: Bronze â†’ Silver â†’ Gold

Time Travel: Historique des donnÃ©es Delta

ğŸ‘¥ Contribution
Fork le projet

CrÃ©er une branche (git checkout -b feature/amÃ©lioration)

Commit les changements (git commit -m 'Ajout feature X')

Push vers la branche (git push origin feature/amÃ©lioration)

Ouvrir une Pull Request

ğŸ“„ Licence
Projet Ã©ducatif - Libre d'utilisation pour l'apprentissage

âœ¨ Auteurs
Adil H. - DÃ©veloppement du pipeline complet

Encadrant - Supervision et validation

ğŸ“… DerniÃ¨re mise Ã  jour: Janvier 2026
ğŸ·ï¸ Version: 1.0.0
âœ… Statut: Pipeline fonctionnel et validÃ©