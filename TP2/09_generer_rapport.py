# ================================================================
# Script : G√©n√©ration de rapport final
# ================================================================

import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import sum, avg, col, min, max
from delta import configure_spark_with_delta_pip
from datetime import datetime

# ================================================================
# FIX WINDOWS
# ================================================================
os.environ["PYSPARK_PYTHON"] = "python"
os.environ["PYSPARK_DRIVER_PYTHON"] = "python"

GOLD_PATH = 'C:/lakehouse/gold'

# ================================================================
# CR√âATION SESSION SPARK (CORRIG√âE)
# ================================================================
builder = SparkSession.builder \
    .appName("Rapport Final") \
    .master("local[*]") \
    .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension") \
    .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog") \
    .config("spark.hadoop.fs.defaultFS", "file:///")

spark = configure_spark_with_delta_pip(builder).getOrCreate()
spark.sparkContext.setLogLevel("WARN")

print("\n" + "="*70)
print("RAPPORT DATA WAREHOUSE - " + datetime.now().strftime("%Y-%m-%d %H:%M"))
print("="*70 + "\n")

print(f"‚úì Session Spark cr√©√©e - Version: {spark.version}")

# ================================================================
# FONCTION PRINCIPALE
# ================================================================
try:
    # ================================================================
    # 1. LIRE DONN√âES GOLD
    # ================================================================
    print("Lecture des donn√©es Gold...")
    df_ventes_quot = spark.read.format("delta").load(f"{GOLD_PATH}/ventes_quotidiennes")
    
    total_jours = df_ventes_quot.count()
    print(f"‚úì Donn√©es charg√©es: {total_jours} jours analys√©s")
    
    # ================================================================
    # 2. STATISTIQUES GLOBALES
    # ================================================================
    stats = df_ventes_quot.agg(
        sum('nb_ventes').alias('total_ventes'),
        sum('ca_total').alias('ca_global'),
        avg('panier_moyen').alias('panier_moyen_global')
    ).collect()[0]
    
    print("\n" + "="*70)
    print("STATISTIQUES GLOBALES")
    print("="*70)
    print(f"\nüìä Total des ventes : {stats['total_ventes']:,}")
    print(f"üí∞ Chiffre d'affaires : {stats['ca_global']:,.2f} ‚Ç¨")
    print(f"üõí Panier moyen : {stats['panier_moyen_global']:,.2f} ‚Ç¨")
    
    # ================================================================
    # 3. TOP 5 MEILLEURS JOURS
    # ================================================================
    print("\n" + "="*70)
    print("TOP 5 MEILLEURS JOURS (par chiffre d'affaires)")
    print("="*70)
    
    df_ventes_quot.select(
        col("date"),
        col("nb_ventes"),
        col("ca_total"),
        col("panier_moyen")
    ).orderBy(col('ca_total').desc()).show(5, truncate=False)
    
    # ================================================================
    # 4. ANALYSE SUPPL√âMENTAIRE
    # ================================================================
    print("\n" + "="*70)
    print("ANALYSE D√âTAILL√âE")
    print("="*70)
    
    # Jour avec le plus de ventes
    jour_max_ventes = df_ventes_quot.orderBy(col('nb_ventes').desc()).first()
    if jour_max_ventes:
        print(f"\nüìà JOUR RECORD (nombre de ventes):")
        print(f"   Date: {jour_max_ventes['date']}")
        print(f"   Nombre de ventes: {jour_max_ventes['nb_ventes']}")
        print(f"   CA: {jour_max_ventes['ca_total']:,.2f} ‚Ç¨")
    
    # P√©riode analys√©e (FIXED LINE)
    dates = df_ventes_quot.agg(
        min(col('date')).alias('premier_jour'),
        max(col('date')).alias('dernier_jour')
    ).collect()[0]
    
    print(f"\nüìÖ P√âRIODE ANALYS√âE:")
    print(f"   Du: {dates['premier_jour']}")
    print(f"   Au: {dates['dernier_jour']}")
    print(f"   Nombre de jours: {total_jours}")
    
    # ================================================================
    # 5. R√âSUM√â DU DATA WAREHOUSE
    # ================================================================
    print("\n" + "="*70)
    print("R√âSUM√â DU DATA WAREHOUSE")
    print("="*70)
    
    print(f"\nüèóÔ∏è  ARCHITECTURE:")
    print(f"   ‚Ä¢ Source: PostgreSQL (retailpro_dwh)")
    print(f"   ‚Ä¢ Bronze: Donn√©es brutes + m√©tadonn√©es")
    print(f"   ‚Ä¢ Silver: Donn√©es nettoy√©es + transformations")
    print(f"   ‚Ä¢ Gold: M√©triques business + agr√©gations")
    
    print(f"\nüìä DONN√âES DISPONIBLES:")
    print(f"   ‚Ä¢ Table Gold: ventes_quotidiennes")
    print(f"   ‚Ä¢ {total_jours} jours d'analyse")
    print(f"   ‚Ä¢ {stats['total_ventes']} ventes totales")
    
    print("\n" + "="*70)
    print("‚úì RAPPORT G√âN√âR√â AVEC SUCC√àS")
    print("="*70)
    
except Exception as e:
    print(f"\n‚ùå ERREUR: {e}")
    print("\nConseils de d√©pannage:")
    print("1. V√©rifiez que la couche Gold existe: python 08_gold_aggregation.py")
    print("2. V√©rifiez le chemin: C:/lakehouse/gold/ventes_quotidiennes")
    print("3. Assurez-vous que Spark est correctement configur√©")
    import traceback
    traceback.print_exc()

finally:
    spark.stop()
    print("\n‚úì Session Spark arr√™t√©e")